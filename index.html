<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Toward Tiny and High-quality Facial Makeup with Data Amplify Learning</title>
  <link rel="icon" type="image/png" href="static/images/logo.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Toward Tiny and High-quality Facial Makeup with Data Amplify Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/TinyBeauty" target="_blank">Qiaoqiao Jin</a>,</span>
                <span class="author-block">
                  <a href="https://github.com/neuralchen" target="_blank">Xuanhong Chen</a>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=oi1LErsAAAAJ&hl=en" target="_blank">Meiguang Jin</a>,
                  </span>
              <span class="author-block">
                    <a href="" target="_blank">Ying Chen</a>,
                  </span>
              <span class="author-block">
                    <a href="" target="_blank">Yucheng Zheng</a>,
                  </span>
              <span class="author-block">
                    <a href="" target="_blank">Yupeng Zhu</a>,
                  </span>
              <span class="author-block">
                    <a href="https://scholar.google.com.sg/citations?user=eUbmKwYAAAAJ&hl=en" target="_blank">Bingbing Ni</a><sup>*</sup>,
                  </span>
                  </div>

<!--                   <div class="is-size-5 publication-authors">
                    <span class="author-block">Anonymous Author</span>
                  </div> -->

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2403.15033.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> 

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/TinyBeauty" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2403.15033" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                <!-- </a> -->
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- <-- Your video here -->
        <source src="static/videos/video1.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Our TinyBeauty effectively synthesizes stunning makeup styles with consistent content, enabling seamless video application.
      </h2>
    </div>
  </div>
</section> 
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Contemporary makeup approaches primarily hinge on unpaired learning paradigms, yet they grapple with the challenges of inaccurate supervision (e.g., face misalignment) and sophisticated facial prompts (including face parsing, and landmark detection).
These challenges prohibit low-cost deployment of facial makeup models, especially on mobile devices.
To solve above problems, we propose a brand-new learning paradigm, termed "Data Amplify Learning (DAL)," alongside a compact makeup model named "TinyBeauty."
The core idea of DAL lies in employing a Diffusion-based Data Amplifier (DDA) to "amplify" limited images for the model training, thereby enabling accurate pixel-to-pixel supervision with merely a handful of annotations.
Two pivotal innovations in DDA facilitate the above training approach:

(1) A Residual Diffusion Model (RDM) is designed to generate high-fidelity detail and circumvent the detail vanishing problem in the vanilla diffusion models;
(2) A Fine-Grained Makeup Module (FGMM) is proposed to achieve precise makeup control and combination while retaining face identity.
Coupled with DAL, TinyBeauty necessitates merely 80K parameters to achieve a state-of-the-art performance without intricate face prompts.
Meanwhile, TinyBeauty achieves a remarkable inference speed of up to 460 fps on the iPhone 13.
Extensive experiments show that DAL can produce highly competitive makeup models using only 5 image pairs.

          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Teaser image-->
<section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/main.png" alt="" style="width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered">
        The Data Amplify Learning process contains two components: (1)A data amplifier which utilizes a pretrained diffusion model to amplify a small set of seed data into a larger synthesized dataset. (2) A lightweight model which is trained on the amplified data to accurately learn the makeup styles while retaining identity features of the original images.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->


<!-- Additional Results-->
<!-- Vid2vid carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3">More Video Results</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="vid2vid_video1" playsinline autoplay muted loop height="90%" src="static/videos/video2_tiny.mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="vid2vid_video2" playsinline autoplay muted loop height="90%" src="static/videos/makeup1.mov">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="vid2vid_video3" playsinline autoplay muted loop height="90%" src="static/videos/makeup2.mov">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End vid2vid carousel -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3">Experimental Results of DDA</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/top.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Our DDA can learn from as few as five images to produce
          diverse, high-quality makeup images. It offers flexible controls, 
          including makeup shade adjustment and custom local editing.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/res_diff.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Our DDA generates 
          consistent makeup styles while retain the facial 
          content and identity of the original image.
        </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<section class="hero is-small">
  <div class="hero-body">
    <div class="container  is-max-desktop">
      <h2 class="title is-3">Experimental Results of TinyBeauty</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/res_FFHQ.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Visual comparison of TineBeauty and competing methods on the FFHQ Dataset.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/MT_res.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Visual comparison of TineBeauty and competing methods on the MT Dataset.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/hrd.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Facial makeup results on high-resolution (1024*1024) images.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/hard.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Visual comparison of TineBeauty and BeautyREC on challenging out-of-distribution examples
      </h2>
    </div>
  </div>
</div>
</div>
</section>



<!-- Method Overview-->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
      </div>
    </div>
    <h1 class="title is-4">Diffusion-based Data Amplifier</h1>
      <div class="content has-text-justified">
        <p>
          To generate high-quality paired makeup data, our Diffusion-based Data Amplifier(DDA) is required to contain the subject's original facial features, skin texture details, such as wrinkles and spots, and consistent, precise makeup styles across various portraits.
          To overcome these obstacles, we introduce a Residual Diffusion Model that preserves texture and detail, reducing distortion and mask-like effects. Moreover, we propose a Fine-Grained Makeup Module to ensure the precise application of makeup to the appropriate facial areas and generate visually consistent makeup styles, as the following shows.
        </p>
      </div>
      <div class="hero-body">
        <img src="static/images/main_02.png" alt="" style="width: 100%; height: auto;">
        <h2 class="subtitle has-text-centered">
          Overview of the Diffusion-based Data Amplifier (DDA).
        </h2>
      </div>
    <h2 class="title is-4">TinyBeauty Model</h2>
    <div class="content has-text-justified">
      <p>
        Benefiting from DDA-generated paired data, the TinyBeauty Model sidesteps previous laborious pre-processing by directly applying L1 loss aligning generated images closely with their targets, which can be designed as a hardware-friendly network optimized for resource-constrained devices.
      </p>
    </div>
    <div class="hero-body">
      <div style="text-align: center;">
        <img src="static/images/Para.jpg" alt="" style="width: 80%; height: auto;">
      </div>
      <!-- <img src="static/images/Para.jpg" alt="" style="width: 80%; height: auto;"> -->
      <h2 class="subtitle has-text-centered">
        Parameters, FLOPs, and runtimes on an iPhone13 of TinyBeauty and competing methods. (+) means the method
        uses facial pre-processing including face parsing, and (-) means the method only has a single model.
      </h2>
    </div>
  </div>
</section>


<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->



<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
